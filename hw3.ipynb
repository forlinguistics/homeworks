{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Повторяем все операции с семинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'}) \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "mallet_path = r'C:/mallet-2.0.8/bin/mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'comp.graphics' 'sci.space'\n",
      " 'talk.politics.guns' 'sci.med' 'comp.sys.ibm.pc.hardware'\n",
      " 'comp.os.ms-windows.misc' 'rec.motorcycles' 'talk.religion.misc'\n",
      " 'misc.forsale' 'alt.atheism' 'sci.electronics' 'comp.windows.x'\n",
      " 'rec.sport.hockey' 'rec.sport.baseball' 'soc.religion.christian'\n",
      " 'talk.politics.mideast' 'talk.politics.misc' 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "data = df.content.values.tolist()\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 5), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Находим оптимальное количество топиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4749060688249831\n",
      "0.49751566319703144\n",
      "0.5174924325861209\n",
      "0.511863488006274\n",
      "0.512143276382778\n",
      "0.4943555338443035\n",
      "0.49112633373427544\n",
      "0.5188622338174146\n",
      "0.5172525517755194\n",
      "0.5427794608635094\n",
      "0.5482295047838456\n",
      "0.5491876168591102\n",
      "0.5599736398176908\n",
      "0.5827577578993325\n",
      "0.5791856061484795\n",
      "0.5796617005645573\n",
      "0.5869745531346334\n",
      "0.5827073440238264\n",
      "0.5777759728314438\n",
      "0.5744897836092532\n",
      "0.5790881892679023\n",
      "0.5715443700372369\n",
      "0.5822979659725986\n",
      "0.5834493793206097\n",
      "0.5868789182011089\n",
      "0.5817085342729525\n",
      "0.5687047801999927\n",
      "0.5824936915474814\n",
      "0.5869745531346334\n"
     ]
    }
   ],
   "source": [
    "def best_model(corpus,id2word):\n",
    "    max_cv = 0\n",
    "    for i in range(2,30):\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word)\n",
    "        coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "        coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "        print(coherence_ldamallet)\n",
    "        if (coherence_ldamallet > max_cv):\n",
    "            max_cv = coherence_ldamallet\n",
    "            best_model = ldamallet\n",
    "    print(max_cv)\n",
    "    return(best_model)\n",
    "ldamallet = best_model(corpus,id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисляем главный топик каждого документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_topics(ldamodel=ldamallet, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "       row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "       for j, (topic_num, prop_topic) in enumerate(row):\n",
    "           if j == 0: # => dominant topic\n",
    "              wp = ldamodel.show_topic(topic_num)\n",
    "              topic_keywords = \", \".join([word for word, prop in wp])\n",
    "              sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "           else:\n",
    "              break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "df_topic_sents_keywords = dominant_topics(\n",
    "   ldamodel=ldamallet, corpus=corpus, texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "   'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.1318</td>\n",
       "      <td>car, power, light, high, ground, engine, wire,...</td>\n",
       "      <td>[where, thing, car, nntp_poste, host, park, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1912</td>\n",
       "      <td>drive, system, card, problem, bit, driver, wor...</td>\n",
       "      <td>[si, poll, final, summary, final, call, si, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>good, make, thing, time, bad, hear, write, lot...</td>\n",
       "      <td>[question, engineering, computer, network, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>write, line, article, host, nntp_poste, organi...</td>\n",
       "      <td>[division, line, host, amber, write, write, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>find, line, problem, call, time, book, number,...</td>\n",
       "      <td>[question, organization, smithsonian_astrophys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11309</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.3302</td>\n",
       "      <td>study, drug, result, effect, food, science, pr...</td>\n",
       "      <td>[migraine, city, ny_bis, reply, line, cheap, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11310</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>drive, system, card, problem, bit, driver, wor...</td>\n",
       "      <td>[problem, screen, blank, sometimes, minor, phy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11311</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.1513</td>\n",
       "      <td>car, power, light, high, ground, engine, wire,...</td>\n",
       "      <td>[este, mount, case, organization, mail, group,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>find, line, problem, call, time, book, number,...</td>\n",
       "      <td>[line, nntp_poste, host, article, write, boy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11313</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>car, power, light, high, ground, engine, wire,...</td>\n",
       "      <td>[gun, steal, organization, line, distribution_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0                0            16.0              0.1318   \n",
       "1                1             9.0              0.1912   \n",
       "2                2            14.0              0.1474   \n",
       "3                3            17.0              0.1125   \n",
       "4                4             0.0              0.1283   \n",
       "...            ...             ...                 ...   \n",
       "11309        11309            10.0              0.3302   \n",
       "11310        11310             9.0              0.2037   \n",
       "11311        11311            16.0              0.1513   \n",
       "11312        11312             0.0              0.2989   \n",
       "11313        11313            16.0              0.0997   \n",
       "\n",
       "                                                Keywords  \\\n",
       "0      car, power, light, high, ground, engine, wire,...   \n",
       "1      drive, system, card, problem, bit, driver, wor...   \n",
       "2      good, make, thing, time, bad, hear, write, lot...   \n",
       "3      write, line, article, host, nntp_poste, organi...   \n",
       "4      find, line, problem, call, time, book, number,...   \n",
       "...                                                  ...   \n",
       "11309  study, drug, result, effect, food, science, pr...   \n",
       "11310  drive, system, card, problem, bit, driver, wor...   \n",
       "11311  car, power, light, high, ground, engine, wire,...   \n",
       "11312  find, line, problem, call, time, book, number,...   \n",
       "11313  car, power, light, high, ground, engine, wire,...   \n",
       "\n",
       "                                                    Text  \n",
       "0      [where, thing, car, nntp_poste, host, park, li...  \n",
       "1      [si, poll, final, summary, final, call, si, cl...  \n",
       "2      [question, engineering, computer, network, dis...  \n",
       "3      [division, line, host, amber, write, write, ar...  \n",
       "4      [question, organization, smithsonian_astrophys...  \n",
       "...                                                  ...  \n",
       "11309  [migraine, city, ny_bis, reply, line, cheap, a...  \n",
       "11310  [problem, screen, blank, sometimes, minor, phy...  \n",
       "11311  [este, mount, case, organization, mail, group,...  \n",
       "11312  [line, nntp_poste, host, article, write, boy, ...  \n",
       "11313  [gun, steal, organization, line, distribution_...  \n",
       "\n",
       "[11314 rows x 5 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## считаем tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def computeIDF(documents):\n",
    "    #для того, чтобы посчитать idf надо подать все документы в функцию сразу \n",
    "    #напишите функцию, считающую idf для каждого слова - на входе массив из numOfWords для всех текстов \n",
    "    #на выходе словарь для слов \n",
    "    #см. ввод и вывод ниже\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0], 0)\n",
    "    for document in documents: # \n",
    "        for word in set(document):\n",
    "            if word in idfDict.keys():\n",
    "                idfDict[word] += 1\n",
    "            else:\n",
    "                idfDict[word] = 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tf_idf (doc,idf_dict):\n",
    "    tfDict = {}\n",
    "    un = set(doc)\n",
    "    num_of_words = dict.fromkeys(un, 0)\n",
    "    for word in doc:\n",
    "        num_of_words[word] += 1\n",
    "    word_num = len(doc)\n",
    "    for word, count in num_of_words.items():\n",
    "        tfDict[word] = count / float(word_num)\n",
    "    tf_idf_dict = {}\n",
    "    for word in tfDict.keys():\n",
    "        tf_idf_dict[word] = tfDict[word]*idf_dict[word]\n",
    "    sorted_tuples = sorted( tf_idf_dict.items(), key=lambda item: item[1],reverse = True)\n",
    "    return(sorted_tuples[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_list = []\n",
    "for topic in range(0,18):\n",
    "    test_docs = df_dominant_topic[df_dominant_topic['Dominant_Topic']==topic][['Document_No','Text']]\n",
    "    idf_dict =computeIDF(list(test_docs['Text']))\n",
    "    for row in test_docs.itertuples():  \n",
    "        fin_list.append([row[1],find_tf_idf(row[2],idf_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=['doc_id','top_tf_idfs']\n",
    "fin_df = pd.DataFrame(fin_list,columns = column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат датафрейм с 3 наибольшими tf_idf для каждого документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>top_tf_idfs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>[(error, 0.16969958012429018), (warn, 0.121022...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86</td>\n",
       "      <td>[(ctrltest, 0.11301080502751795), (assumption,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125</td>\n",
       "      <td>[(rotfl, 0.17925851831951123), (second, 0.1016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175</td>\n",
       "      <td>[(font, 0.2962380488728496), (symptom, 0.25615...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>188</td>\n",
       "      <td>[(phone, 0.19721471733119506), (operator, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11230</td>\n",
       "      <td>[(burning, 0.4977046781026118), (verdict, 0.41...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11232</td>\n",
       "      <td>[(laboratory, 0.42579793682926326), (slow, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11241</td>\n",
       "      <td>[(business, 0.5303447783699939), (answer, 0.42...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11256</td>\n",
       "      <td>[(loopback_connector, 0.44793421029235064), (p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11273</td>\n",
       "      <td>[(efficient, 0.5168471657219431), (accountable...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id                                        top_tf_idfs\n",
       "0           4  [(error, 0.16969958012429018), (warn, 0.121022...\n",
       "1          86  [(ctrltest, 0.11301080502751795), (assumption,...\n",
       "2         125  [(rotfl, 0.17925851831951123), (second, 0.1016...\n",
       "3         175  [(font, 0.2962380488728496), (symptom, 0.25615...\n",
       "4         188  [(phone, 0.19721471733119506), (operator, 0.16...\n",
       "...       ...                                                ...\n",
       "11309   11230  [(burning, 0.4977046781026118), (verdict, 0.41...\n",
       "11310   11232  [(laboratory, 0.42579793682926326), (slow, 0.4...\n",
       "11311   11241  [(business, 0.5303447783699939), (answer, 0.42...\n",
       "11312   11256  [(loopback_connector, 0.44793421029235064), (p...\n",
       "11313   11273  [(efficient, 0.5168471657219431), (accountable...\n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
