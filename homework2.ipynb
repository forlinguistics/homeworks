{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестирования пос тэггеров возьмем отрывки из рассказа \"Цветы для Элджернона\" на русском и английском, в которых слова намеренно написаны с орфографиескими ошибками"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тэггинг русского текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Барт сказал Чярли што ты видиш на этом листке. Я видил пролитые чирнила и очень испугался хотя заечья лапка была у меня в кармане потомушто когда я был маленький я всегда плохо учился и проливал чирнила. Я сказал Барту я вижу чирнила пролитые на белый листок. Барт сказал да и улыбнулся и мне стало харашо. Он переварачевал все листки и я гаварил ему ктото разлил на них чорные и красные чирнила. Я думал это лехко но когда я встал штобы идти Барт сказал садись Чярли мы ещо не кончили. Мы ещо не все сделали с листками. Я не понел но я помнил што док Штраус сказал делай все што тебе скажут даже если не понемаеш потомушто это тест.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "rus_w = []\n",
    "natasha_pos = []\n",
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    if token.pos != 'PUNCT':\n",
    "        rus_w.append(token.text)\n",
    "        natasha_pos.append(token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "m = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_pos = []\n",
    "for token in text.split():\n",
    "    pymorphy_pos.append(str(m.parse(token)[0].tag.POS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystm = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana = mystm.analyze(text)\n",
    "mystem_pos = []\n",
    "for word in ana:\n",
    "    if 'analysis' in word:\n",
    "        if not word['analysis'] == []:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            mystem_pos.append(pos)\n",
    "        else:\n",
    "            mystem_pos.append('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## тэггинг английского текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 ='I had a test today. I think I faled it. And I think that maybe now they wont use me. What happind is a nice young man was in the room and he had some 19 white cards with ink spillled all over them. He sed Charlie what do you see ton this card. I was very skared even tho I had my rabits foot in my pockit because when I was a kid I always faled tests in school and I spillled ink to. I told him I saw a inkblot. He said yes and it made me feel good. I thot that was all but when I got up to go he stopped me. He said now sit down Charlie we are not thru yet. Then I dont remember so good but he wantid me to say what was in the ink. I dint see nuthing in the ink but he said there was picturs there other pepul saw some picturs. I coudnt see any picturs. I reely tryed to see. I held the card close up and then far away. Then I said if I had my glases I coud see better I usally only ware my glases in the movies or TV but I said they are in the closh in the hall. I got them. Then I said let me see that card agen I bet Ill find it now.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Загружаем весь пайплайн для английского\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Обрабатываем текст\n",
    "sp_doc = nlp(text2)\n",
    "spacy_pos = []\n",
    "# Выведем токены, леммы и теги\n",
    "for i, s in enumerate(sp_doc.sents):\n",
    "    for t in s:\n",
    "        if t.pos_ !='PUNCT' and t.text !='nt':\n",
    "            spacy_pos.append((t.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-07 23:19:44,244 loading file C:\\Users\\Michael\\.flair\\models\\en-pos-ontonotes-v0.5.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "# use splitter to split text into list of sentences\n",
    "sentences = splitter.split(text2)\n",
    "# predict tags for sentences\n",
    "tagger = SequenceTagger.load('pos')\n",
    "tagger.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_pos = []\n",
    "for sentence in sentences:\n",
    "    for token in sentence:\n",
    "        pos = token.get_tag('pos').value\n",
    "        if pos!='.':\n",
    "            flair_pos.append((token.get_tag('pos').value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_w=[]\n",
    "import nltk\n",
    "nltk_pos = []\n",
    "tknz = nltk.word_tokenize(text2)\n",
    "for i in nltk.pos_tag(tknz):\n",
    "    if i[0]!='.':\n",
    "        nltk_pos.append(i[1])\n",
    "        en_w.append(i[0])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приводим все теги к одному формату "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И в русском и в английском я старался приводить все теги к тегам библиотек, в которых их меньше всего. В русском я выбрал mystem, в английском spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_unif = []\n",
    "for i in natasha_pos:\n",
    "    if i == 'PROPN':\n",
    "        nat_unif.append('S')\n",
    "        continue\n",
    "    if i == 'PRON':\n",
    "        nat_unif.append('SPRO')\n",
    "        continue\n",
    "    if i == 'VERB':\n",
    "        nat_unif.append('V')\n",
    "        continue\n",
    "    if i == 'ADP':\n",
    "        nat_unif.append('PR')\n",
    "        continue\n",
    "    if i == 'DET':\n",
    "        nat_unif.append('APRO')\n",
    "        continue\n",
    "    if i == 'CCONJ' or i == 'SCONJ':\n",
    "        nat_unif.append('CONJ')\n",
    "        continue\n",
    "    if i == 'NOUN':\n",
    "        nat_unif.append('S')\n",
    "        continue\n",
    "    if i == 'ADJ':\n",
    "        nat_unif.append('A')\n",
    "        continue\n",
    "    if i == 'AUX':\n",
    "        nat_unif.append('V')\n",
    "        continue\n",
    "    nat_unif.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_unif = []\n",
    "for i in pymorphy_pos:\n",
    "    if i == 'ADVB':\n",
    "        pymorphy_unif.append('ADV')\n",
    "        continue\n",
    "    if i == 'PRON':\n",
    "        pymorphy_unif.append('SPRO')\n",
    "        continue\n",
    "    if i == 'VERB':\n",
    "        pymorphy_unif.append('V')\n",
    "        continue\n",
    "    if i == 'PREP':\n",
    "        pymorphy_unif.append('PR')\n",
    "        continue\n",
    "    if i == 'DET':\n",
    "        pymorphy_unif.append('APRO')\n",
    "        continue\n",
    "    if i == 'NOUN':\n",
    "        pymorphy_unif.append('S')\n",
    "        continue\n",
    "    if i == 'ADJS' or i == 'ADJF':\n",
    "        pymorphy_unif.append('A')\n",
    "        continue\n",
    "    if i == 'NPRO':\n",
    "        pymorphy_unif.append('SPRO')\n",
    "        continue\n",
    "    if i == 'PRTF' or i =='INFN':\n",
    "        pymorphy_unif.append('V')\n",
    "        continue\n",
    "    if i =='None':\n",
    "        pymorphy_unif.append('U')\n",
    "        continue\n",
    "    if i =='PRCL':\n",
    "        pymorphy_unif.append('PART')\n",
    "        continue\n",
    "    pymorphy_unif.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>V</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADV</td>\n",
       "      <td>CONJ</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPRO</td>\n",
       "      <td>SPRO</td>\n",
       "      <td>SPRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>PART</td>\n",
       "      <td>PART</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>V</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>V</td>\n",
       "      <td>A</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>APRO</td>\n",
       "      <td>PART</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>S</td>\n",
       "      <td>U</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2\n",
       "0       S     S     S\n",
       "1       V     V     V\n",
       "2       S     V     S\n",
       "3     ADV  CONJ     S\n",
       "4    SPRO  SPRO  SPRO\n",
       "..    ...   ...   ...\n",
       "113  PART  PART  PART\n",
       "114     V     S     S\n",
       "115     V     A   ADV\n",
       "116  APRO  PART  PART\n",
       "117     S     U     S\n",
       "\n",
       "[118 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(list(zip(nat_unif,pymorphy_unif,mystem_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eng_unif(tag_list):\n",
    "    unif_list = []\n",
    "    for i in tag_list:\n",
    "        if i =='CC':\n",
    "            unif_list.append('CCONJ')\n",
    "            continue\n",
    "        if i =='CD':\n",
    "            unif_list.append('NUM')\n",
    "            continue\n",
    "        if i =='DT':\n",
    "            unif_list.append('DET')\n",
    "            continue\n",
    "        if i =='EX':\n",
    "            unif_list.append('PRON')\n",
    "            continue\n",
    "        if i =='IN':\n",
    "            unif_list.append('ADP')\n",
    "            continue\n",
    "        if i =='JJ' or i == 'JJR':\n",
    "            unif_list.append('ADJ')\n",
    "            continue\n",
    "        if i =='NN' or i =='NNS':\n",
    "            unif_list.append('NOUN')\n",
    "            continue\n",
    "        if i =='NNP':\n",
    "            unif_list.append('PROPN')\n",
    "            continue\n",
    "        if i =='PRP$':\n",
    "            unif_list.append('DET')\n",
    "            continue\n",
    "        if i =='PRP'or i=='WP':\n",
    "            unif_list.append('PRON')\n",
    "            continue\n",
    "        if i =='RB' or i=='WRB' or i=='RBR':\n",
    "            unif_list.append('ADV')\n",
    "            continue\n",
    "        if i =='RP'or i == 'TO':\n",
    "            unif_list.append('PART')\n",
    "            continue\n",
    "        if i in ['VB','VBD','VBG','VBN','VBP','VBZ','MD']:\n",
    "            unif_list.append('VERB')\n",
    "            continue\n",
    "        unif_list.append(i)  \n",
    "    return(unif_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_unif = eng_unif(nltk_pos)\n",
    "flair_unif = eng_unif(flair_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUX</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>PROPN</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>UH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2\n",
       "0     PRON  PRON  PRON\n",
       "1      AUX  VERB  VERB\n",
       "2      DET   DET   DET\n",
       "3     NOUN  NOUN  NOUN\n",
       "4     NOUN  NOUN  NOUN\n",
       "..     ...   ...   ...\n",
       "230   VERB  VERB  VERB\n",
       "231  PROPN   ADJ    UH\n",
       "232   VERB  VERB  VERB\n",
       "233   PRON  PRON  PRON\n",
       "234    ADV   ADV   ADV\n",
       "\n",
       "[235 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(list(zip(spacy_pos,nltk_unif,flair_unif)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моя POS разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rus_pos = ['S',\n",
    " 'V',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'PR',\n",
    " 'APRO',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'V',\n",
    " 'S',\n",
    " 'CONJ',\n",
    " 'ADV',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'A',\n",
    " 'S',\n",
    " 'V',\n",
    " 'PR',\n",
    " 'SPRO',\n",
    " 'PR',\n",
    " 'S',\n",
    " 'CONJ',\n",
    " 'ADVPRO',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'A',\n",
    " 'SPRO',\n",
    " 'ADVPRO',\n",
    " 'ADV',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'V',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'S',\n",
    " 'V',\n",
    " 'PR',\n",
    " 'A',\n",
    " 'S',\n",
    " 'S',\n",
    " 'V',\n",
    " 'PART',\n",
    " 'CONJ',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'APRO',\n",
    " 'S',\n",
    " 'CONJ',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'SPRO',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'PR',\n",
    " 'SPRO',\n",
    " 'A',\n",
    " 'CONJ',\n",
    " 'A',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'SPRO',\n",
    " 'ADV',\n",
    " 'CONJ',\n",
    " 'ADVPRO',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'V',\n",
    " 'S',\n",
    " 'V',\n",
    " 'V',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'PART',\n",
    " 'PART',\n",
    " 'V',\n",
    " 'SPRO',\n",
    " 'U',\n",
    " 'PART',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'PR',\n",
    " 'S',\n",
    " 'SPRO',\n",
    " 'PART',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'CONJ',\n",
    " 'S',\n",
    " 'S',\n",
    " 'V',\n",
    " 'V',\n",
    " 'SPRO',\n",
    " 'CONJ',\n",
    " 'SPRO',\n",
    " 'V',\n",
    " 'PART',\n",
    " 'CONJ',\n",
    " 'PART',\n",
    " 'S',\n",
    " 'ADV',\n",
    " 'SPRO',\n",
    " 'S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eng_pos = ['PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'SCONJ',\n",
    " 'ADV',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'AUX',\n",
    " 'DET',\n",
    " 'ADJ',\n",
    " 'ADJ',\n",
    " 'NOUN',\n",
    " 'AUX',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NUM',\n",
    " 'ADJ',\n",
    " 'NOUN',\n",
    " 'ADP',\n",
    " 'NOUN',\n",
    " 'VERB',\n",
    " 'ADV',\n",
    " 'ADP',\n",
    " 'PRON',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PROPN',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'ADV',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'NOUN',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'SCONJ',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'NOUN',\n",
    " 'ADP',\n",
    " 'NOUN',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'NOUN',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'INTJ',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'ADJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'AUX',\n",
    " 'DET',\n",
    " 'CCONJ',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'ADP',\n",
    " 'PART',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'ADP',\n",
    " 'PROPN',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'PART',\n",
    " 'ADP',\n",
    " 'ADV',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'VERB',\n",
    " 'ADV',\n",
    " 'ADJ',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'PART',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'VERB',\n",
    " 'VERB',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'NOUN',\n",
    " 'ADV',\n",
    " 'ADJ',\n",
    " 'NOUN',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'PART',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'ADV',\n",
    " 'ADP',\n",
    " 'CCONJ',\n",
    " 'ADV',\n",
    " 'ADV',\n",
    " 'ADP',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'SCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'ADV',\n",
    " 'ADV',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'CCONJ',\n",
    " 'NOUN',\n",
    " 'CCONJ',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'AUX',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'ADP',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'DET',\n",
    " 'NOUN',\n",
    " 'ADV',\n",
    " 'PRON',\n",
    " 'VERB',\n",
    " 'PROPN',\n",
    " 'VERB',\n",
    " 'PRON',\n",
    " 'ADV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natasha accuracy : 0.8220338983050848\n",
      "Pymorphy accuracy : 0.7457627118644068\n",
      "Mystem accuracy : 0.8728813559322034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Natasha accuracy : {}'.format(np.sum(np.array(my_rus_pos)==np.array(nat_unif))/len(nat_unif)))\n",
    "print('Pymorphy accuracy : {}'.format(np.sum(np.array(my_rus_pos)==np.array(pymorphy_unif))/len(nat_unif)))\n",
    "print('Mystem accuracy : {}'.format(np.sum(np.array(my_rus_pos)==np.array(mystem_pos))/len(nat_unif)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для английского:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy accuracy : 0.948936170212766\n",
      "NLTK accuracy : 0.8595744680851064\n",
      "Flair accuracy : 0.8723404255319149\n"
     ]
    }
   ],
   "source": [
    "print('Spacy accuracy : {}'.format(np.sum(np.array(my_eng_pos)==np.array(spacy_pos))/len(nltk_unif)))\n",
    "print('NLTK accuracy : {}'.format(np.sum(np.array(my_eng_pos)==np.array(nltk_unif))/len(nltk_unif)))\n",
    "print('Flair accuracy : {}'.format(np.sum(np.array(my_eng_pos)==np.array(flair_unif))/len(nltk_unif)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть точности могла потеряться в приведении к одной разметке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
